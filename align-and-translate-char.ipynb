{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read the preprocressed corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('middleresult/tokenlizer_output_60000ch_60000en_40words_token.pkl','rb') as fhdl:\n",
    "    (\n",
    "         ind2ch,\n",
    "         ch2ind,\n",
    "         ind2en,\n",
    "         en2ind,\n",
    "         train_x,\n",
    "         train_y,\n",
    "    ) = pickle.load(fhdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8823, 60000)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ind2ch),len(ind2en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define the model hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_vocab_size = 50003#len(ind2en) + 3\n",
    "target_vocat_size = 8826#len(ind2ch) + 3\n",
    "attention_hidden_size = 512\n",
    "attention_output_size = 512\n",
    "embedding_size = 512\n",
    "seq_max_len = 40\n",
    "num_units = 512\n",
    "batch_size = 64\n",
    "layer_number = 2\n",
    "max_grad = 1.0\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925196, 1925196)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),len(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process the model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = tf.contrib.keras.preprocessing.sequence.pad_sequences(train_x,seq_max_len,padding='post')\n",
    "train_y = tf.contrib.keras.preprocessing.sequence.pad_sequences(train_y,seq_max_len,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who ' ve lived there with their wonderful way of living                             \n",
      "他 们 原 本 上 百 年 来 在 森 林 里 和 谐 安 居                        \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "index = random.randint(0,len(train_x))\n",
    "print(' '.join([ind2en.get(i,'') for i in train_x[index]]))\n",
    "print(' '.join([ind2ch.get(i,'') for i in train_y[index]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1925196, 40), (1925196, 40))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape,train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x,test_x,train_y,test_y = train_test_split(train_x,train_y , test_size=0.01, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x[train_x >= src_vocab_size] = 1 \n",
    "test_x[test_x >= src_vocab_size] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y[train_y >= target_vocat_size] = 1 \n",
    "test_y[test_y >= target_vocat_size] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_last_index = np.max(train_x)\n",
    "y_last_index = np.max(train_y)\n",
    "\n",
    "ind2en[x_last_index] = '<go>'\n",
    "ind2ch[y_last_index] = '<go>'\n",
    "en2ind['<go>'] = x_last_index\n",
    "ch2ind['<go>'] = y_last_index\n",
    "\n",
    "ind2en[1] = '<unk>'\n",
    "ind2ch[1] = '<unk>'\n",
    "en2ind['<unk>'] = 1\n",
    "ch2ind['<unk>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "licence authorizing the use of a factory situated in hong kong for treating whales                          \n",
      "批 准 在 位 於 香 港 的 工 厂 加 工 处 理 鲸 的 牌 照                      \n",
      "here ' s what ' s different about people . we have the same needs ,                        \n",
      "这 就 是 人 们 不 一 样 的 地 方 。 我 们 有 同 样 的 需 求 ，                   \n",
      "this said , ... he wished to have me in his sight / once , as a friend : this fixed a day in spring               \n",
      "我 膝 上 . 这 封 说 : 他 多 盼 望 有 个 机 会 , / 能 作 为 朋 友 , 见 一 见 我 . 这 一 封 又 订 了     \n",
      "the issue occurs when authenticated mysql users overwrite arbitrary files by using a <unk> attack .                        \n",
      "题 存 在 于 经 过 身 份 验 证 的 m y s q l 用 户 利 用 s y m l i n k 攻 击 覆 盖 任 意 文 件 的 过 程 中 。\n",
      "watch your back , fish , ' cause squirrel master ain ' t gonna be there for you all the time .                  \n",
      "小 心 点 ， 鱼 ， 斯 葵 尔 玛 斯 特 不 会 总 照 着 你 的 。                    \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for i in range(5):\n",
    "    index = random.randint(0,len(train_x))\n",
    "    print(' '.join([ind2en.get(i,'') for i in train_x[index]]))\n",
    "    print(' '.join([ind2ch.get(i,'') for i in train_y[index]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define the translate nmt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.layers import core as layers_core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(log_device_placement=True,allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "\n",
    "with tf.device('/gpu:1'):\n",
    "    initializer = tf.random_uniform_initializer(\n",
    "        -0.08, 0.08)\n",
    "    tf.get_variable_scope().set_initializer(initializer)\n",
    "    \n",
    "    x = tf.placeholder(\"int32\", [None, None])\n",
    "    y = tf.placeholder(\"int32\", [None, None])\n",
    "    y_in = tf.placeholder(\"int32\",[None,None])\n",
    "    x_len = tf.placeholder(\"int32\",[None])\n",
    "    y_len = tf.placeholder(\"int32\",[None])\n",
    "    x_real_len = tf.placeholder(\"int32\",[None])\n",
    "    y_real_len = tf.placeholder(\"int32\",[None])\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "    \n",
    "    # embedding\n",
    "    embedding_encoder = tf.get_variable(\n",
    "        \"embedding_encoder\", [src_vocab_size, embedding_size],dtype=tf.float32)\n",
    "    embedding_decoder = tf.get_variable(\n",
    "        \"embedding_decoder\", [target_vocat_size, embedding_size],dtype=tf.float32)\n",
    "    \n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_encoder, x)\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(\n",
    "        embedding_decoder, y_in)\n",
    "    \n",
    "    # encoder\n",
    "    num_bi_layers = int(layer_number / 2)\n",
    "    cell_list = []\n",
    "    for i in range(num_bi_layers):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        encoder_cell = cell_list[0]\n",
    "    else:\n",
    "        encoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "        \n",
    "    cell_list = []\n",
    "    \n",
    "    for i in range(num_bi_layers):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        encoder_backword_cell = cell_list[0]\n",
    "    else:\n",
    "        encoder_backword_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "    \n",
    "    bi_outputs, bi_encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "        encoder_cell,encoder_backword_cell, encoder_emb_inp,\n",
    "        sequence_length=x_len,dtype=tf.float32)\n",
    "    encoder_outputs = tf.concat(bi_outputs, -1)\n",
    "    \n",
    "    if num_bi_layers == 1:\n",
    "        encoder_state = bi_encoder_state\n",
    "    else:\n",
    "        encoder_state = []\n",
    "        for layer_id in range(num_bi_layers):\n",
    "            encoder_state.append(bi_encoder_state[0][layer_id])  # forward\n",
    "            encoder_state.append(bi_encoder_state[1][layer_id])  # backward\n",
    "        encoder_state = tuple(encoder_state)\n",
    "    \n",
    "    # decoder \n",
    "    #decoder_cell = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "    cell_list = []\n",
    "    for i in range(layer_number):\n",
    "        cell_list.append(\n",
    "            tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(num_units), input_keep_prob=(1.0 - dropout)\n",
    "            )\n",
    "        )\n",
    "    if len(cell_list) == 1:\n",
    "        decoder_cell = cell_list[0]\n",
    "    else:\n",
    "        decoder_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "    \n",
    "    # Helper\n",
    "    \n",
    "    # attention\n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "        attention_hidden_size, encoder_outputs,\n",
    "        memory_sequence_length=x_real_len,scale=True)\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        decoder_cell, attention_mechanism,\n",
    "        attention_layer_size=attention_output_size)\n",
    "    \n",
    "    \n",
    "    projection_layer = layers_core.Dense(\n",
    "        target_vocat_size, use_bias=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Dynamic decoding\n",
    "    with tf.variable_scope(\"decode_layer\"):\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "            decoder_emb_inp,sequence_length= y_len)\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, helper, initial_state = decoder_cell.zero_state(dtype=tf.float32,batch_size=batch_size),\n",
    "            output_layer=projection_layer)\n",
    "       \n",
    "        outputs, _,___  = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "        logits = outputs.rnn_output\n",
    "\n",
    "        target_weights = tf.sequence_mask(\n",
    "            y_real_len, seq_max_len, dtype=logits.dtype)\n",
    "    \n",
    "    # predicting\n",
    "    # Helper\n",
    "    with tf.variable_scope(\"decode_layer\", reuse=True):\n",
    "        helper_predict = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n",
    "            embedding_decoder,\n",
    "            tf.fill([batch_size], ch2ind['<go>']), 0)\n",
    "        decoder_predict = tf.contrib.seq2seq.BasicDecoder(\n",
    "            decoder_cell, helper_predict, initial_state = decoder_cell.zero_state(dtype=tf.float32,batch_size=batch_size),\n",
    "            output_layer=projection_layer)\n",
    "        outputs_predict,_, __ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder_predict, maximum_iterations=test_y.shape[1] * 2)\n",
    "    translations = outputs_predict.sample_id\n",
    "\n",
    "    # calculate loss\n",
    "    crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y, logits=logits)\n",
    "    train_loss = (tf.reduce_sum(crossent * target_weights) /\n",
    "        batch_size)\n",
    "    \n",
    "    optimizer_ori = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    trainable_params = tf.trainable_variables()\n",
    "    gradients = tf.gradients(train_loss, trainable_params)\n",
    "    clip_gradients, _ = tf.clip_by_global_norm(gradients, max_grad)\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    optimizer = optimizer_ori.apply_gradients(\n",
    "            zip(clip_gradients, trainable_params), global_step=global_step)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(train_loss)\n",
    "    #trainop = tflearn.TrainOp(loss=train_loss, optimizer=optimizer,\n",
    "    #                          metric=train_loss, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_acc(logits,target):\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target[:,:seq_max_len], logits[:,:seq_max_len]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from middleresult/align/result_1_20847\n"
     ]
    }
   ],
   "source": [
    "#saver.restore(session,'middleresult/align/result_1_20847')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'middleresult/result_char'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(session,'middleresult/result_char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import Dataset,ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "train_set = Dataset(train_x,train_y)\n",
    "test_set = Dataset(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bleu_score(predict,target):\n",
    "    try:\n",
    "        target = [[[j for index,j in enumerate(i) if j > 0 or index < 4]] for i in target]\n",
    "        predict = [[j for index,j in enumerate(i) if j > 0 or index < 4] for i in predict]\n",
    "        BLEUscore = nltk.translate.bleu_score.corpus_bleu(target,predict)\n",
    "    except:\n",
    "        BLEUscore = -1\n",
    "    return BLEUscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calc_test_loss(test_set = Dataset(test_x,test_y),display=True):\n",
    "    accs = []\n",
    "    worksum = int(len(test_x) / batch_size)\n",
    "    loss_list = []\n",
    "    predict_list = []\n",
    "    target_list = []\n",
    "    source_list = []\n",
    "    pb = ProgressBar(worksum=worksum,info=\"validating...\",auto_display=display)\n",
    "    pb.startjob()\n",
    "    #test_set = Dataset(test_x,test_y)\n",
    "    for j in range(worksum):\n",
    "        batch_x,batch_y = test_set.next_batch(batch_size)\n",
    "        lx = [seq_max_len] * batch_size\n",
    "        ly = [seq_max_len] * batch_size\n",
    "        bx = [np.sum(m > 0) for m in batch_x]\n",
    "        by = [np.sum(m > 0) for m in batch_y]\n",
    "        tmp_loss,tran = session.run([train_loss,translations],feed_dict={x:batch_x,y:batch_y,\n",
    "                                                     y_in:\n",
    "                                                     np.concatenate((\n",
    "                                                     np.ones((batch_y.shape[0],1),dtype=np.int) * ch2ind['<go>'],batch_y[:,:-1]) ,axis=1)\n",
    "                                                     ,x_len:lx,y_len:ly,\n",
    "                                                                        y_real_len:by,\n",
    "                                                                        x_real_len:bx})\n",
    "        loss_list.append(tmp_loss)\n",
    "        tmp_acc = cal_acc(tran,batch_y)\n",
    "        accs.append(tmp_acc)\n",
    "        predict_list += [i for i in tran]\n",
    "        target_list += [i for i in batch_y]\n",
    "        source_list += [i for i in batch_x]\n",
    "        pb.complete(1)\n",
    "    return np.average(loss_list),np.average(accs),get_bleu_score(predict_list,target_list),predict_list,target_list,source_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate the initial loss and see what model outputs in the begining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating... 100.00 % [==================================================>] 300/300 \t used:120s eta:0 s"
     ]
    }
   ],
   "source": [
    "w_loss,w_acc,bleu_score,predict_list,target_list,source_list = calc_test_loss(Dataset(train_x[::100],train_y[::100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(233.5325, 5.5989583333333333e-05, 0.004130308764851706)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_loss,w_acc,bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_text(x):\n",
    "    return [' '.join([ind2ch.get(j,'') for j in i]) for i in x]\n",
    "def get_all_en_text(x):\n",
    "    return [' '.join([ind2en.get(j,'') for j in i]) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['紮 紮 紮 紮 紮 麈 紮 麈 麈 麈 麈 麈 麈 麈 麈 黉 黉 佗 佗 黉 黉 黉 眃 眃 眃 眃 眃 眃 眃 眃 眃 格 格 格 格 贤 贤 格 ㏑ 格 格 格 格 格 ㏑ 格 格 ㏑ ㏑ ㏑ ㏑ ㏑ ㏑ ㏑ ㏑ ㏑ ㏑ ㏑ ㏑ 衲 衲 衲 衲 衲 衲 衲 衲 衲 衲 衲 衲 衲 衲 衲 衲 挪 衲 衲 衲 衲',\n",
       " '間 間 間 間 煜 煜 煜 臆 前 前 前 蝚 蝚 靘 μ ； 讫 讫 讫 讫 讫 讫 讫 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 鞑 蝚 铲 蝚 蝚 蝚 铲 铲 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 铲 铲 铲 铲 铲 铲 铲 铲 睦 睦 睦 睦 睦 睦 睦 睦 睦 睦 睦 睦 睦 睦 睦 睦',\n",
       " '馓 戴 籁 庥 龋 龋 语 д ４ 囊 砖 气 气 气 气 气 气 气 气 气 气 气 气 气 气 粕 民 民 民 民 民 民 赔 民 筽 民 迹 迹 迹 迹 迹 迹 迹 鰶 犸 犸 犸 犸 犸 天 天 天 天 猴 猴 炫 猴 猴 洐 洐 猴 腋 钩 钩 钩 钩 钩 马 \\ue009 臻 臻 臻 臻 \\ue009 \\ue009 \\ue009 \\ue009 \\ue009 \\ue009 \\ue009',\n",
       " '呔 呔 浼 浼 浼 浼 浼 浼 驶 里 里 里 里 虬 蝚 蝚 灑 灑 蝚 蝚 蝚 蝚 》 苞 苞 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 涩 钭 钭 钭 钭 钭 钭 钭 钭 鯦 鯦 钭 骄 钭 钭 钭 桕 桕 桕 未 桕 桕 桕 拾 拾 拾 拾 拾 绗 绗 鰶',\n",
       " '庢 庢 庢 臆 臆 臆 臆 臆 磌 磌 磌 磌 磌 衲 衲 衲 衲 衲 衲 衲 衲 衲 衲 衲 衲 衲 衲 衲 衲 衲 衲 衲 衲 衲 \\ue40f 奇 \\ue40f 衲 衲 叻 奇 奇 圈 圈 谜 谜 谜 谜 谜 谜 谜 谜 谜 谜 谜 谜 谜 谜 谜 谜 谜 辆 辆 谜 辆 辆 辆 辆 辆 辆 罙 阍 罙 阍 阍 阍 叻 阍 阍 叻',\n",
       " '轰 轰 轰 浼 癞 浼 臆 臆 臆 臆 臆 臆 臆 臆 臆 臆 铜 臆 臆 臆 臆 臆 灑 灑 灑 灑 冼 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 鞑 蝚 鞑 鞑 鞑 蝚 蝚 蝚 蝚 蝚 匙 匙 匙 匙 匙 匙 蝚 蝚 蝚 蝚 蝚 蝚',\n",
       " '潵 間 潵 轰 轰 轰 \\ue620 局 \\ue620 辙 辙 辙 辙 辙 辙 辙 辙 讫 讫 甡 ㏑ 甡 甡 甡 》 》 》 》 箴 箴 》 》 》 》 》 豪 》 》 》 》 》 》 》 》 》 》 》 豪 杯 杯 茧 茧 茧 茧 簺 揄 簺 茧 奇 奇 奇 奇 奇 奇 奇 奇 奇 奇 奇 挢 挢 挢 醪 绗 绗 绗 绗 晦 里 里',\n",
       " '呔 呔 呔 呔 圞 拊 里 里 里 臆 臆 臆 算 算 算 臆 殭 殭 苞 苞 苞 苞 苞 苞 苞 苞 苞 苞 苞 里 里 苞 苞 苞 苞 苞 袈 衲 衲 桼 衲 衲 衲 衲 蝚 黔 黔 黔 黔 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 橹 匙 蝚 蝚 蝚 蝚 蝚 桼 畄 庇 庇 庇 庇 庇 庇 庇 庇 庇',\n",
       " '間 間 間 浼 杯 杯 杯 臆 臆 臆 臆 臆 讫 讫 臆 灑 臆 讫 灑 讫 蝚 蝚 佗 梢 佗 佗 佗 佗 佗 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 蝚 贤 贤 贤 贤 贤 黔 黔 せ 黔 碹 碹 碹 碹 碹 碹 碹 碹 碹 碹 珂 珂 叿 叿 芙 芙 芙 叿 叿 芙 芙 芙',\n",
       " 'k k 厉 厉 厉 厉 厉 里 蓆 蓆 蓆 拾 蓆 拾 拾 堤 叿 叿 拾 叿 叿 叿 叿 叿 叿 叿 叿 増 锇 锇 锇 锇 锇 锇 糗 蘧 蘧 蘧 蘧 蘧 蘧 虽 虽 虽 虽 虽 虽 虽 虽 虽 虽 虽 虽 虽 虽 虽 虽 虽 虽 虽 虽 虽 \\ue677 \\ue677 庚 庚 庚 庚 庚 嗙 癇 庚 癇 庚 癇 像 像 像 像 像']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = get_all_text(predict_list)\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['不 然 的 话 ， 我 把 你 屁 股 捣 烂 - - 噢 ， 上 帝 啊                     ',\n",
       " '医 生 给 她 注 射 以 减 轻 疼 痛                             ',\n",
       " '“ 我 们 希 望 中 国 能 够 慎 重 考 虑 很 多 谈 判 伙 伴 本 周 表 达 的 关 切 ， 调 整 立 场 ， 及 早 恢 复 谈 判 。 ”',\n",
       " '《 牛 津 当 代 大 辞 典 》 词 库 1 0 0 % 采 用 真 人 女 声 。                  ',\n",
       " '定 一 个 牌 手 基 于 对 手 给 他 的 错 误 信 息 采 取 了 行 动 ， 将 酌 情 按 第 2 1 条 或 第 4 7 条 e 款 处 理 。',\n",
       " '易 腐 产 品 标 准 化 和 质 量 改 进 工 作 队                         ',\n",
       " '本 品 具 有 优 越 的 匀 染 效 果 ， 优 越 的 耐 硬 水 及 耐 盐 性 。                 ',\n",
       " '由 通 知 行 签 发 的 本 信 用 证 正 本 通 知 书 ， 因 为 有 效 信 用 证 必 须 由 指 定 银 行 在 该 通 知 书 上 背 书 。',\n",
       " 'r s s 根 据 应 用 的 地 址 将 中 断 指 向 特 定 的 处 理 器 内 核 。                ',\n",
       " '） 报 送 上 市 公 司 收 购 报 告 书 时 所 持 有 被 收 购 公 司 股 份 数 占 该 公 司 已 发 行 的 股 份 总 数 的 比 例 。']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = get_all_text(target_list)\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tran.shape\n",
    "i_save = 0\n",
    "j_save = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(i_save,j_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = 'align_char'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.mkdir('middleresult/{}'.format(model_path))\n",
    "os.mkdir('eval/{}'.format(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating... 100.00 % [==================================================>] 300/300 \t used:106s eta:0 s7445/29780 \t used:1984s eta:5951 s\n",
      "iter 13 step 7445 train loss 51.49441909790039 train acc 0.3890546875 test loss 56.52604675292969 test acc 0.3802877604166666 bleu 0.20683722390776155 lr 0.0625\n",
      "\n",
      "validating... 100.00 % [==================================================>] 300/300 \t used:105s eta:0 s14890/29780 \t used:4461s eta:4460 s\n",
      "iter 13 step 14890 train loss 51.49875259399414 train acc 0.38941145833333335 test loss 56.567684173583984 test acc 0.38301171875000006 bleu 0.2169776407879747 lr 0.03125\n",
      "\n",
      "validating... 100.00 % [==================================================>] 300/300 \t used:103s eta:0 s22335/29780 \t used:6938s eta:2312 ss\n",
      "iter 13 step 22335 train loss 51.54439163208008 train acc 0.39030338541666665 test loss 56.541290283203125 test acc 0.3805 bleu 0.22007911262894841 lr 0.03125\n",
      "\n",
      "validating... 100.00 % [==================================================>] 300/300 \t used:104s eta:0 s 7445/29780 \t used:2240s eta:6721 ss\n",
      "iter 14 step 7445 train loss 51.224857330322266 train acc 0.3899205729166667 test loss 56.82951736450195 test acc 0.378453125 bleu 0.21533934461057255 lr 0.015625\n",
      "\n",
      "validating... 100.00 % [==================================================>] 300/300 \t used:104s eta:0 s] 14890/29780 \t used:4706s eta:4706 s\n",
      "iter 14 step 14890 train loss 51.121070861816406 train acc 0.3904127604166666 test loss 56.48078155517578 test acc 0.3814088541666667 bleu 0.218569247826193 lr 0.0078125\n",
      "\n",
      "validating... 100.00 % [==================================================>] 300/300 \t used:106s eta:0 s] 22335/29780 \t used:7177s eta:2392 ss\n",
      "iter 14 step 22335 train loss 51.33269500732422 train acc 0.3897239583333333 test loss 56.58401107788086 test acc 0.38013671875 bleu 0.21346233125359212 lr 0.0078125\n",
      "\n",
      "validating... 100.00 % [==================================================>] 300/300 \t used:101s eta:0 s-] 7445/29780 \t used:2236s eta:6709 ss\n",
      "iter 15 step 7445 train loss 51.140235900878906 train acc 0.39041406250000005 test loss 56.441165924072266 test acc 0.379875 bleu 0.22126377277611756 lr 0.00390625\n",
      "\n",
      "validating... 100.00 % [==================================================>] 300/300 \t used:105s eta:0 s-] 14890/29780 \t used:4700s eta:4700 ss\n",
      "iter 15 step 14890 train loss 51.05939865112305 train acc 0.3896588541666667 test loss 56.73957061767578 test acc 0.38069791666666664 bleu 0.21723252877675084 lr 0.001953125\n",
      "\n",
      "validating... 100.00 % [==================================================>] 300/300 \t used:101s eta:0 s--] 22335/29780 \t used:7168s eta:2389 ss\n",
      "iter 15 step 22335 train loss 51.1295166015625 train acc 0.3886432291666666 test loss 56.51940155029297 test acc 0.3808815104166666 bleu 0.2187056862095337 lr 0.001953125\n",
      "\n",
      "validating... 100.00 % [==================================================>] 300/300 \t used:107s eta:0 s----] 7445/29780 \t used:2228s eta:6686 s\n",
      "iter 16 step 7445 train loss 51.121768951416016 train acc 0.3897473958333333 test loss 56.849037170410156 test acc 0.37901822916666666 bleu 0.21269532562586332 lr 0.0009765625\n",
      "\n",
      "validating... 100.00 % [==================================================>] 300/300 \t used:106s eta:0 s---] 14890/29780 \t used:4695s eta:4695 ss\n",
      "iter 16 step 14890 train loss 51.05976486206055 train acc 0.38977734375 test loss 56.51028823852539 test acc 0.3819075520833333 bleu 0.21460443822701872 lr 0.00048828125\n",
      "\n",
      "validating... 100.00 % [==================================================>] 300/300 \t used:105s eta:0 s-----] 22335/29780 \t used:7164s eta:2388 s\n",
      "iter 16 step 22335 train loss 51.14641189575195 train acc 0.3891145833333333 test loss 56.565284729003906 test acc 0.38264583333333335 bleu 0.21811796139827241 lr 0.00048828125\n",
      "\n",
      "validating... 100.00 % [==================================================>] 300/300 \t used:106s eta:0 s-----] 7445/29780 \t used:2229s eta:6687 ss\n",
      "iter 17 step 7445 train loss 51.078975677490234 train acc 0.3900143229166667 test loss 56.53923034667969 test acc 0.37984375 bleu 0.2164484593871533 lr 0.000244140625\n",
      "\n",
      "validating... 100.00 % [==================================================>] 300/300 \t used:103s eta:0 s------] 14890/29780 \t used:4702s eta:4701 s\n",
      "iter 17 step 14890 train loss 51.1338005065918 train acc 0.3888997395833334 test loss 56.62047576904297 test acc 0.3807604166666667 bleu 0.21641832416101056 lr 0.0001220703125\n",
      "\n",
      "validating... 100.00 % [==================================================>] 300/300 \t used:103s eta:0 s------] 22335/29780 \t used:7172s eta:2390 ss\n",
      "iter 17 step 22335 train loss 51.171836853027344 train acc 0.39005989583333334 test loss 56.64115905761719 test acc 0.3802877604166667 bleu 0.21857356474866954 lr 0.0001220703125\n",
      "\n",
      "iter 17 loss:52.313621520996094 lr:0.0001220703125 81.34 % [========================================>----------] 24223/29780 \t used:7968s eta:1827 s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-c16cad0b5ffe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                                                                 np.ones((batch_y.shape[0],1),dtype=np.int) * ch2ind['<go>'],batch_y[:,:-1]) ,axis=1)\n\u001b[1;32m     28\u001b[0m                                                                 \u001b[1;33m,\u001b[0m\u001b[0my_real_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                                                                 \u001b[0mx_real_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                                                                })\n\u001b[1;32m     31\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epoch = 15\n",
    "restore = True\n",
    "#lr = 1\n",
    "for i in range(i_save,n_epoch):\n",
    "    \n",
    "    i_save = i\n",
    "    worksum = int(len(train_y)/batch_size)\n",
    "    pb = ProgressBar(worksum=worksum)\n",
    "    pb.startjob()\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    for j in range(worksum):\n",
    "        if restore == True and j < j_save:\n",
    "            pb.finishsum += 1\n",
    "            continue\n",
    "        restore = False\n",
    "        \n",
    "        j_save = j\n",
    "        batch_x,batch_y = train_set.next_batch(batch_size)\n",
    "        lx = [seq_max_len] * batch_size\n",
    "        ly = [seq_max_len] * batch_size\n",
    "        bx = [np.sum(m > 0) for m in batch_x]\n",
    "        by = [np.sum(m > 0) for m in batch_y]\n",
    "        by =[m + 2  if m < seq_max_len - 1 else m for m in by ]\n",
    "        _, loss = session.run([optimizer,train_loss],feed_dict={x:batch_x,y:batch_y,x_len:lx,y_len:ly,learning_rate:lr,y_in:\n",
    "                                                                np.concatenate((\n",
    "                                                                np.ones((batch_y.shape[0],1),dtype=np.int) * ch2ind['<go>'],batch_y[:,:-1]) ,axis=1)\n",
    "                                                                ,y_real_len:by,\n",
    "                                                                x_real_len:bx\n",
    "                                                               })\n",
    "        train_loss_list.append(loss)\n",
    "        #tmp_train_acc = cal_acc(tran,batch_y)\n",
    "        #train_acc_list.append(tmp_train_acc)\n",
    "        pb.info = \"iter {} loss:{} lr:{}\".format(i + 1,loss,lr)\n",
    "        val_step = int(worksum / 4)\n",
    "        if j % val_step == 0 and j != 0:\n",
    "            test_loss,test_acc,bleu_score,predict_list,target_list,source_list = calc_test_loss()\n",
    "            _,train_acc,train_bleu_score,train_predict_list,train_target_list,train_source_list = calc_test_loss(Dataset(train_x[::100],train_y[::100]),display=False)\n",
    "            predict_texts = get_all_text(predict_list)\n",
    "            target_texts = get_all_text(target_list)\n",
    "            source_texts = get_all_en_text(source_list)\n",
    "            \n",
    "            train_predict_texts = get_all_text(train_predict_list)\n",
    "            train_target_texts = get_all_text(train_target_list)\n",
    "            train_source_texts = get_all_en_text(train_source_list)\n",
    "            \n",
    "            with open('eval/{}/{}_{}_predict'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in predict_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_target'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in target_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_source'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in source_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "                    \n",
    "            with open('eval/{}/{}_{}_predict_train'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in train_predict_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_target_train'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in train_target_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            with open('eval/{}/{}_{}_source_train'.format(model_path,i + 1,j),'w',encoding='utf-8') as whdl:\n",
    "                for line in train_source_texts:\n",
    "                    whdl.write(\"{}\\n\".format(line))\n",
    "            print(\"\\niter {} step {} train loss {} train acc {} test loss {} test acc {} bleu {} lr {}\\n\".format(i+1,j,np.average(train_loss_list[-val_step:]),train_acc,test_loss,test_acc,bleu_score,lr))\n",
    "            try:\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session,'middleresult/{}/result_{}_{}'.format(model_path,i + 1,j))\n",
    "            except:\n",
    "                print('save fail')\n",
    "        lr_step = int(worksum / 2) - 1\n",
    "        if j % lr_step == 0 and j != 0:\n",
    "            if (i + 1) > 10:\n",
    "                lr = lr / 2\n",
    "        pb.complete(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x228b0a0bf60>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([ind2en.get(i,'') for i in test_x[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent = 'you are too stupid to know that you are an idiot .'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'are',\n",
       " 'too',\n",
       " 'stupid',\n",
       " 'to',\n",
       " 'know',\n",
       " 'that',\n",
       " 'you',\n",
       " 'are',\n",
       " 'an',\n",
       " 'idiot',\n",
       " '.']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = [en2ind.get(i) for i in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = tf.contrib.keras.preprocessing.sequence.pad_sequences([sents],seq_max_len,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  13,   31,  198, 3197,    7,   83,   17,   13,   31,   35, 7122,\n",
       "           3,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 13,  31, 198, ...,   0,   0,   0],\n",
       "       [ 13,  31, 198, ...,   0,   0,   0],\n",
       "       [ 13,  31, 198, ...,   0,   0,   0],\n",
       "       ..., \n",
       "       [ 13,  31, 198, ...,   0,   0,   0],\n",
       "       [ 13,  31, 198, ...,   0,   0,   0],\n",
       "       [ 13,  31, 198, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(sents,35,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sents[0] > 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tran = session.run([translations],feed_dict={x:np.repeat(sents,64,axis=0),x_len:[35] * 64, x_real_len:[sum(sents[0] > 0) + 1] * 64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('你太傻了，知道你是个白痴', 34),\n",
       " ('你太傻了，不知道你是个白痴', 8),\n",
       " ('你太傻了，你就知道你是个白痴', 6),\n",
       " ('你太愚蠢了，知道你是个白痴', 2),\n",
       " ('你太傻了，知道你是白痴', 2)]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(''.join([ind2ch.get(i,'') for i in j]) for j in tran[0]).most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50002, 8824)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en2ind['<go>'],ch2ind['<go>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# release the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file release already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.mkdir('release/align_and_translate_char_50000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'release/align_and_translate_char_50000/align_and_translate_model'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(session,'release/align_and_translate_char_50000/align_and_translate_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('release/align_and_translate_char_50000/dic.pkl','wb') as whdl:\n",
    "    pickle.dump(\n",
    "        ( \n",
    "            ind2ch,\n",
    "            ch2ind,\n",
    "            ind2en,\n",
    "            en2ind,\n",
    "        ),whdl,protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
